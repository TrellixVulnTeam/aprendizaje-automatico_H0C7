# Generar conjuntos de entrenamiento
# Regresión de la forma  de dos lunas

# 1.From make_moons database, generate a set of 10000 samples.
# Moons is for binary classification and will generate a swirl pattern, or two moons.
from sklearn.datasets import make_moons

X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)

# 12.Divide the generated database in two sets.
# dividir database en trainning set y test set
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 13. Use grid search with cross-validation (with the help of the
# GridSearchCV class) to find good hyper parameter values for a
# DecisionTreeClassifier. Hint: try various values for max_leaf_nodes
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

params = {
    'max_leaf_nodes': list(range(2, 100)),
    'min_samples_split': [2],
    'criterion': ['entropy']
}
grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), params, verbose=1, cv=3)
grid_search_cv.fit(X_train, y_train)

print(grid_search_cv.best_estimator_)

# Generar predicción
# 15.Train it on the full training set using these hyperparameters andmeasure your model's performance on the test set.
# You should get roughly 85% to 87% accuracy.

from sklearn.metrics import accuracy_score

y_pred = grid_search_cv.predict(X_test)
result = accuracy_score(y_test, y_pred)
print(result)

cont = 0
for x in range(len(y_pred)):
    # print(y_test[x], "\t", y_pred[x])
    if y_test[x] == y_pred[x]:
        cont += 1

print(f'The model classifies: {cont} instances correctly')

# 13.Use grid search with cross-validation (with the help of the GridSearchCV class)to find
# Generar diccionario de parametros donde se especifica la clave que es el nombre del parametro
# que se esta variando la lista de todos los valores
# posibles que pueda tener el critero
# good hyperparameter values for a DecisionTreeClassifier. Hint: try various values for max_leaf_nodes.
# grid_search crea una rejilla con todas las combinaciones posibles (tipo clasificador, parametros, verbose = salida en consola, 3 iteraciones)

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

params = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4],
          'criterion': ['entropy']
          }  # 'max_depth': list(range(2, 6))

grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), params, verbose=1, cv=3)
grid_search_cv.fit(X_train, y_train)

# 14.Print the best configuration found
# mejor configuracion para no tantear un arbol, mejor accuracy contra prueba de entrenamiento
print(grid_search_cv.best_estimator_)
'''
DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,
                       max_features=None, max_leaf_nodes=33,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort=False,
                       random_state=42, splitter='best')
'''

# 15.Train it on the full training set using these hyper parameters and
# measure your model's performance on the test set. You should get roughly 85% to 87% accuracy.
from sklearn.metrics import accuracy_score

y_pred = grid_search_cv.predict(X_test)
result = accuracy_score(y_test, y_pred)
print(result)

# 23. From the previous models generated by Blobs and moons, construct the confusion matrices.

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

class_names = grid_search_cv.best_estimator_


def plot_confusion_matrix(y_true, y_pred, normalize=False, title=None, cmap=plt.cm.Blues):
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    classes = ['x1', 'x2']
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')
    print(cm)
    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes,
           yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')
    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(),
             rotation=45,
             ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white"
                    if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax


np.set_printoptions(precision=2)
# Plot non-normalized confusion matrix
plot_confusion_matrix(y_test, y_pred, title='Confusion matrix, without normalization')

# Plot normalized confusion matrix
plot_confusion_matrix(y_test, y_pred, normalize=True, title='Normalized confusion matrix')
plt.show()
